{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP6 - Clustering : Recommandations de livres\n",
    "\n",
    "Dans cette séance on s'intéresse au dataset issue de [wonderbk.com](wonderbk.com) un site de vente de livre en ligne. Nous allons réaliser des clusters de livre pour constuire un système de recommandation de livre.\n",
    "Les informations dont on dispose sont :\n",
    "* **Title** : titre du livre\n",
    "* **Authors** : Auteur du livre\n",
    "* **Description** : Description du livre\n",
    "* **Category** : Catégorie ou genre du livre\n",
    "* **Publisher** : Maison d'édition du livre\n",
    "* **Price Starting With ($)** : Prix initial du livre\n",
    "* **Publish Date** : Date de la publication du livre\n",
    "\n",
    "Commençons par importer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"BooksDataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage\n",
    "\n",
    "On note que le dataset n'est pas exploitable en l'état, nous allons devoir faire un travail de mise en qualité avant de pouvoir construire le système de recommandation. Commençons par la colonne *Price*.\n",
    "\n",
    "**Consigne** : Nettoyer la colonne *Price* pour la rendre numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Constuire la colonne *Publish Year* qui correspond à l'année de publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Afficher la répartition du nombre de livre publié par an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Ne sélectionner que les livres publiés entre 1950 et 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait que la colonne *Authors* commence systématiquement par *By_*. \n",
    "\n",
    "**Consigne** : Vérifier si c'est effectivement le cas, et nettoyer la colonne en conséquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse avant de continuer aux valeurs manquantes. Pour construire notre système de recommandation, on souhaite en avoir aucune.\n",
    "\n",
    "**Consigne** : Après avoir identifier avec la méthode [`isna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html) les colonnes contenant des valeurs manquantes, les supprimer avec la méthode [`dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite pouvoir exploiter la colonne *Category*. Pour le faire, nous allons faire un one hot encoding, mais nous devons le faire sur chaque categorie. Commençons par calculer la fréquence d'apparition de chaque catégorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "\n",
    "for row in range(df.shape[0]):\n",
    "    if type(df[\"Category\"][row]) != str:\n",
    "        continue\n",
    "    for category in df[\"Category\"][row].split(\" , \"):\n",
    "        category = category.strip()\n",
    "        if category in categories.keys():\n",
    "            categories[category] += 1\n",
    "        else:\n",
    "            categories[category] = 1\n",
    "\n",
    "categories = dict(sorted(categories.items(), key=lambda x: x[1], reverse=True))\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Sélectionner les 20 premiers types les plus fréquent, puis créer autant de colonne valant 1 si le livre correspond à la catégorie, 0 sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploitation des descriptions\n",
    "\n",
    "Beaucoup d'informations sont comprises dans ces lignes, et nous devons être capable de le transformer en nombre. Commençons par de l'analyse de sentiments. Il existe plusieurs librairies permettant de faire cela, la première est [TextBlob](https://textblob.readthedocs.io/en/dev/) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "texts = [\"Theo always give great lecture\", \"Theo is reallty not a good lecturer\", \"Theo is OK at his job\"]\n",
    "for text in texts:\n",
    "    blob = TextBlob(text)\n",
    "    print(f\"Analyse de \\\"{text}\\\" : {blob.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre est [VADER](https://vadersentiment.readthedocs.io/en/latest/) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for text in texts:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    print(f\"Analyse de \\\"{text}\\\" : {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite intégrer ces informations dans notre datasets.\n",
    "\n",
    "**Consigne** : Créer une colonne *Description_sentiment_Blob* qui correspond à la valeur *polarity* calculée par TextBlob et une autre colonne *Description_sentiment_VADER* qui correspond à la valeur *compound* calculée par VADER. Faire de même avec la colonne *Title*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite étudier la distribution et la corrélation entre les scores, pour le titre et la description des livres.\n",
    "\n",
    "**Consigne** : Afficher sur un même graphique la distribution des scores pour la description entre TextBlob et VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Afficher un scatter plot entre les score de TextBlob et de VADER pour les deux colonnes concernées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des clusters\n",
    "\n",
    "Nous avons maintenant accès à un dataset permettant d'exploiter du Machine Learning non supervisé. Puisque les résolutions des problèmes que les algorithmes vont résoudre peuvent exploiter des variantes d'une descente de gradient, nous devons préparer en conséquence la matrice.\n",
    "\n",
    "**Consigne** : A partir du dataset *df*, ne conserver que les colonnes numériques puis avec la classe [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) standardiser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : A l'aide de la classe [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) calculer les clusters et les stocker dans une variable *labels*. On prendra *n_clusters=5*. Finalement, afficher le nombre d'observations dans chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Calculer la performance du clustering avec la fonction [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite a présent avoir un peu d'explicabilité pour ces clusters. Pour le faire, nous allons reporter les labels appris dans le dataset initial.\n",
    "\n",
    "**Consigne** : Construire un dataset *data* à partir de *df*, et y ajouter le vecteur *labels*. On supprimera des catégories OHE pour la lisibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Explorer un des clusters et essayer de comprendre la constitution du cluster. On pourra utiliser la méthode `describe` pour les données numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons à présent l'algorithme DBSCAN. Nous n'avons pas cette fois à spécifier le nombre de cluster que l'on souhaite obtenir. Nous devons donc commencer par identifier les informations que l'on souhaite avoir.\n",
    "\n",
    "**Consigne** : Définir une fonction `explore_clusters` qui prend en paramètre la matrice $X$ et les labels calculés. Elle affichera :\n",
    "* Le silhouette score\n",
    "* Le nombre de cluster\n",
    "* La répartition des observations dans les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne**: Entraîner avec les valeurs par défaut l'algorithme [`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) puis utiliser la fonction `explore_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne**: A l'aide du cours et par tatônnement entraîner DBSCAN avec d'autres paramètrages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
